# About This Report


```{r hello-random, echo=FALSE}
pxtablek <- function(x, ...) { 
	if(class(x)=='list'){
  		print(xtable::xtable(x[[1]], digits=x$digits, caption=paste('</br>',x$caption)  ),type='html', ...) 
  	}else{
  		print(xtable::xtable(x),type='html', ...) 
  	}
}
opts_chunk$set(echo=FALSE, results='asis',fig.path='tempFiguresForKnitrReport/', fig.width=9, fig.height=7)
```

```{r}
#BUGS? hrefs aren't working, ask leo? some of the Eqs work in RStudio, but not via knit()?
#This is just a chunk for testing it in RStudio
#source('/Users/aaronfisher/Documents/JH/Michael - Shiny App /new_gui/EAGLE Repo/eagle_gui/shinyApp/Adaptive_Group_Sequential_Design.R')
#-table1 <- table_constructor()
```


```{r results='hide', message=FALSE}
#Citations
library(knitcitations)
bibFile<-read.bibtex('interAdapt_noJSScode.bib') #file without \pkg{} and \progLang{} markup
cite_options(linked=FALSE) #You don't have the full links in your bib file, so the links aren't working.
```



This report was created using the *interAdapt* software for generating and analyzing trial designs with adaptive enrollment criteria. *interAdapt* can be accessed online at

http://spark.rstudio.com/mrosenblum/interAdapt

Additional documentation for the *interAdapt*, including instructions on how to download the application for offline use, can be found at

https://rawgithub.com/aaronjfisher/interAdapt/master/About_interAdapt.pdf

### Table of Contents:
* <a href="#Introduction"> Introduction </a>
* <a href="#Full List of Inputs"> Full List of Inputs </a>
* <a href="#Decision Boundaries"> Decision Boundaries </a>
* <a href="#Performance Comparison Plots"> Performance Comparsion Plots </a>
* <a href="#Performance Comparison Table"> Performance Comparison Table </a>
* <a href="#Formal Description of the Problem"> Formal Description of the Problem </a>
	* <a href="#Hypotheses"> Hypotheses </a>
	* <a href="#Test Statistics"> Test Statistics </a>
	* <a href="#Type I Error Control"> Type I Error Control </a>
	* <a href="#Decision rules for stopping the trial early and for modifying enrollment criteria"> Decision rules for stopping the trial early and for modifying enrollment criteria </a>
* <a href="#Inputs"> Inputs</a>
  * <a href="#Basic Parameters"> Basic Parameters</a>
  * <a href="#Advanced Parameters"> Advanced Parameters</a>
* <a href="#References"> References </a>



*************



# <a name="Introduction">Introduction</a>

In this report, we consider the scenario where we have prior evidence that the treatment might work better in a one subpopulation than another. We use the term "adaptive design" to refer to a group sequential design that starts by enrolling from both subpopulations, and then decides whether or not to continue enrolling from each subpopulation based on interim analyses.  We use the term "standard designs" to refer to group sequential designs where the enrollment criteria are fixed.

Below, we describe an adaptive design in more detail, and compare the performance of this design to the performance of standard designs. Performance is compared in terms of expected sample size, expected trial duration, and power, with family-wise type I error rate set to be constant (`r alpha_FWER_user_defined`) for all trials.



*******

# <a name="Full List of Inputs">Full List of Inputs</a>

```{r}
inputVec<-matrix(rep(NA,length=length(allVarNames)),ncol=1)
for(i in 1:length(allVarNames)) inputVec[i]<- input[[ allVarNames[i] ]]
rownames(inputVec)<-allVarLabels
pxtablek(inputVec,include.colnames=FALSE)
```

*******
# <a name="Decision Boundaries">Decision Boundaries</a>


```{r}
boundary_adapt_plot()
```
```{r}
pxtablek(adaptive_design_sample_sizes_and_boundaries_table())
```
*******
```{r}
boundary_standard_H0C_plot()
```
```{r}
pxtablek(standard_H0C_design_sample_sizes_and_boundaries_table())
```
*******
```{r}
boundary_standard_H01_plot()
```
```{r}
pxtablek(standard_H01_design_sample_sizes_and_boundaries_table())
```
*******


# <a name="Performance Comparison Plots">Performance Comparison Plots</a>

```{r}
power_curve_plot()
```
*******
```{r}
expected_sample_size_plot()
```
*******
```{r}
expected_duration_plot()
```
*******


# <a name="Performance Comparison Table">Performance Comparison Table</a>
```{r}
ptab<-transpose_performance_table(performance_table())
pxtablek(ptab,include.colnames=FALSE)
```




**********


# <a name="Formal Description of the Problem">Formal Description of the Problem</a>

We consider the problem of testing whether a new treatment is superior to control.
Consider the case where we have two subpopulations, referred to as subpopulation $1$ and subpopulation $2$. These must be specified before the trial starts, and be defined in terms of participant attributes measured at baseline (e.g., having a high initial severity of disease or a certain biomarker value). 
We focus on situations where  there is suggestive, prior evidence that the treatment may be more likely to benefit subpopulation $1$.
In the MISTIE trial example, subpopulation 1 refers to small IVH participants, and subpopulation 2 refers to large IVH participants.
Let $π_1$ and $π_2$ denote the proportion of participants in subpopulations 1 and 2, respectively. 


Both the adaptive and standard designs discussed here involve enrollment over time, and include predetermined rules for stopping the trial early based on interim analyses. Each trial consists of $K$ stages, indexed by $k$. We say that the $k^{th}$ stage has ended once a certain number of additional patients ($n_k$) have been enrolled.
In stages when both subpopulations are enrolled, we assume that the proportion of newly recruited participants  in each subpopulation $s \in \{1,2\}$ is equal to the corresponding population proportion $\pi_s$. 

Let $Y_{i,k}$ be a binary outcome variable for the $i^{th}$ participant recruited in stage $k$, where $Y_{i,k}=1$ indicates a successful outcome. Let $T_{i,k}$ be an indicator of   the $i^{th}$ participant recruited in stage $k$ being assigned to the treatment. We assume there is an equal probability of being assigned to  treatment or control.

For subpopulation $1$, denote the probability of a successful outcome under treatment as $p_{1t}$, and the probability of a successful outcome under control as $p_{1c}$. Similarly for population $2$, let $p_{2t}$ denote the probability of a success under treatment, and $p_{2c}$ denote the probability of a success under control. 
We assume each of $p_{1c},p_{1t},p_{2c},p_{2t}$ is in the interval $(0,1)$.
We define the true average treatment effect for a given population to be the difference in the probability of a successful outcome comparing treatment versus control.


In the remainder of this section we give an overview of the relevant concepts needed to understand and use *interAdapt*. A more detailed discussion of the theoretical context, and of the parameter calculation procedure, is provided by `r citep(bibFile[['Rosenblum2013AdaptMISTIE']])`.
 
## <a name="Hypotheses">Hypotheses</a>

We focus on testing the null hypothesis that, on average, the treatment is no better than control for subpopulation $1$, and the analogous null hypothesis for the combined population. These two null hypotheses are defined, respectively, as


* $H_{01}$: $p_{1t}-p_{1c}≤0$;
* $H_{0C}$: $π_1(p_{1t}-p_{1c}) + π_2(p_{2t}-p_{2c}) ≤ 0$. 


*interAdapt* compares different designs for testing these null hypotheses. 
An adaptive design testing both null hypotheses is compared to a standard design testing only $H_{0C}$, and to a standard design testing only $H_{01}$. 
We refer to the adaptive design as $AD$, and refer to these two standard designs as $SC$ and $SS$, respectively. All three trials contain $K$ stages, and the decision to entirely stop the trial early can be made at the end of any stage. The trials differ in that $SC$ and $SS$ never change their enrollment criteria, while $AD$ may switch to enroll only participants from subpopulation $1$.

Note that the standard designs discussed here are not the same as those discussed in section 6.1 of `r citep(bibFile[['Rosenblum2013AdaptMISTIE']])`, which test both hypothesis simultaneously. Implementing standard designs such as those discussed in `r citep(bibFile[['Rosenblum2013AdaptMISTIE']])` into the *interAdapt* software is an area of future research.


## <a name="Test Statistics">Test Statistics</a>

Three z-statistics are computed at the end of each stage $k$. The first is based on all enrolled participants in the combined population, the second is based on all enrolled participants in subpopulation 1, and the third is based on all enrolled participants in subpopulation 2.  Each z-statistic is a standardized difference in sample means, comparing outcomes in the treatment arm versus the control arm.
Let $Z_{C,k}$ denote the z-statistic for the combined population, which  takes the following form:

\[
Z_{C,k}=\left( \frac{\sum_{k'=1}^k \sum_{i=1}^{n_{k'}}Y_{i,k'}T_{i,k'} }
{\sum_{k'=1}^k \sum_{i=1}^{n_{k'}}T_{i,k'}} - 
\frac{\sum_{k'=1}^k \sum_{i=1}^{n_{k'}} Y_{i,k'}(1-T_{i,k'})} 
{\sum_{k'=1}^k \sum_{i=1}^{n_{k'}}(1-T_{i,k'})}\right)V_{C,k}^{-1/2}
\]


The term in square brackets is the difference in sample means between the treatment and control groups, and $V_{C,k}$ is the variance of this difference in sample means:

\[
V_{C,k}=
\left(     \frac{2}{  \sum_{k'=1}^{k} n_{k'}  } \right)
\left(
\sum_{s ∈ \{ 1,2\}} π_s[p_{sc}(1-p_{sc}) + p_{st}(1-p_{st})]
\right)
\]


Let $Z_{1,k}$ and $Z_{2,k}$ denote analogous z-statistics restricted to participants in subpopulation $1$ and $2$ respectively. The z-statistic for subpopulation 1 can be written as follows, where $A_{i,k}$ is the indicator that the $i ^{th}$ subject recruited in stage $k$ is in subpopulation $1$:

\[
Z_{1,k}=\left[
\frac{\sum_{k'=1}^k \sum_{i=1}^{n_{k'}}Y_{i,k'}T_{i,k'}A_{i,k'} }
{\sum_{k'=1}^k \sum_{i=1}^{n_{k'}}T_{i,k'}A_{i,k'}} -
\frac{\sum_{k'=1}^k \sum_{i=1}^{n_{k'}} Y_{i,k'}(1-T_{i,k'})A_{i,k'}} 
{\sum_{k'=1}^k \sum_{i=1}^{n_{k'}}(1-T_{i,k'})A_{i,k'}}
\right] V_{1,k}^{-1/2} 
\]

 
where

\[
 V_{1,k}=
\left(     \frac{2}{\sum_{k'=1}^k \sum_{i=1}^{n_{k'}}A_{i,k'}}       \right)
\left(
π_1[p_{1c}(1-p_{1c}) + p_{1t}(1-p_{1t})]
\right)
\]


The z-statistic $Z_{2,k}$ is similar to the above, with each occurrence of $A_{i,k'}$ replaced by $(1-A_{i,k'})$.

The decision rules defined later on in this section involve boundaries for $(Z_{C,1},Z_{C,2},...Z_{C,K})$, $(Z_{1,1},Z_{1,2},...Z_{1,K})$, and $(Z_{2,1},Z_{2,2},...Z_{2,K})$. To calculate the familywise Type I error of any given set of decision rules, *interAdapt* makes use of the multivariate distribution of $(Z_{C,1},Z_{C,2},...$ $Z_{C,K}, Z_{1,1},Z_{1,2},...$ $Z_{1,K})$, which, under the assumptions in `r citep(bibFile[['Rosenblum2013AdaptMISTIE']])`, is asymptotically normal with a known covariance matrix `r citep(bibFile[['JennisonTurnbullBook']])`.


## <a name="Type I Error Control">Type I Error Control</a>

The familywise Type I error rate is the probability of rejecting one or more true null hypotheses.
For a given design, we say that the familywise Type I error rate is strongly controlled at level $α$ if the probability of rejecting at least one true null hypothesis (among $H_{0C}, H_{01}$) is at most $α$, regardless of the true values of $p_{1c},p_{1t},p_{2c},p_{2t}$.
For all three designs, $AD$, $SC$, and $SS$, we require the familywise Type I error rate to be strongly controlled at level $α$. 
Since the two standard designs $SS$ and $SC$ each only test a single null hypothesis, the familywise Type I error rate for each design is equal to the corresponding Type I error rate for their individual hypothesis tests.




## <a name="Decision rules for stopping the trial early and for modifying enrollment criteria">Decision rules for stopping the trial early and for modifying enrollment criteria</a>

The decision rules for the standard design $SC$ consist of efficacy and futility boundaries for $H_{0C}$. At the end of each stage $k$,  the test statistic $Z_{C,k}$ is calculated. If $Z_{C,k}$ is above the efficacy boundary for stage $k$, we reject $H_{0C}$ and end the trial. If $Z_{C,k}$ is between the efficacy and futility boundaries for stage $k$, we continue the trial. If $Z_{C,k}$ is below the futility boundary for stage $k$, we end the trial with the conclusion that we have failed to reject $H_{0C}$. *interAdapt* makes the simplification that the number of participants enrolled in each stage of $SC$ is constant ($n_{SC}$), and allows the user to input this per-stage sample size.

The efficacy boundaries for $SC$ are set to be proportional to those described by Wang and Tsiatis (1987). This means that the efficacy boundary for the $k^{th}$ stage is set to $e_{SC}\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$, where $K$ is the total number of stages, $δ$ is a constant in the range $[-.5,.5]$, and $e_{SC}$ is the constant calibrated to ensure the desired familywise Type I error rate. Since $n_{k}$ is set equal to $n_{SC}$ for all values of $k$, this boundary reduces to $e_{SC}(k/K)^\delta$. By default, *interAdapt* sets $\delta$ to be negative. In order to calculate $e_{SC}$, *interAdapt* makes use of the fact that the random vector of test statistics ($Z_{C,1},Z_{C,2},…Z_{C,K}$) converges asymptotically to a multivariate normal distribution with a known covariance structure `r citep(bibFile[['JennisonTurnbullBook']])`.
Using the *mvtnorm* package `r citep(bibFile[['mvtnorm']])` in R to evaluate the multivariate normal distribution function, *interAdapt* calculates the proportionality constant $e_{SC}$ such that the null probability of $Z_{C,k}$ exceeding $e_{SC}\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$ at any stage $k$ is less than or equal to $α$.

In $SC$, as well as in $SS$ and $AD$, *interAdapt* uses non-binding futility constants. All three designs are calibrated such that familywise Type I error rate is controlled at level α regardless of whether the futility boundaries are ignored. In calculating power however, *interAdapt* does assume that the futility boundaries are adhered to.

Futility boundaries for the first $K-1$ stages of $SC$ are set equal to $f_{SC}\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$, where $f_{SC}$ is a proportionality constant. By default, the constant $f_{SC}$ is set to be negative, although this is not required. In the $K ^{th}$ stage of the trial, *interAdapt* sets the futility bound to be equal to the efficacy bound. This ensures that the final z-statistic $Z_{C,K}$ crosses either the efficacy bound or the futility bound.

The decision boundaries for $Z_{1,k}$ in the $SS$ design are defined by exactly the same form. Again, *interAdapt* makes the simplification that the number of patients enrolled in each stage of $SS$ is constant ($n_{SS}$), and allows the user to input this per-stage sample size. The efficacy boundary for the $k^{th}$ stage is set equal to $e_{SS}\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$, where $e_{SS}$ is the constant that ensures the appropriate Type I error rate. The first $K-1$ futility boundaries for $H_{01}$ are set equal to $f_{SS}\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$,  where $f_{SS}$ is a constant that can be set by the user. The futility boundary in stage $K$ is set equal to the final efficacy boundary in stage $K$.

Decision boundaries for $AD$ vary from those of the standard designs two ways. First, because $AD$ simultaneously tests $H_{0C}$ and $H_{01}$ it has two sets of decision boundaries. For the $k^{th}$ stage of $AD$, let $u_{C,k}$ and $u_{1,k}$ denote the efficacy boundaries for $H_{0C}$ and $H_{01}$ respectively. The boundaries $u_{C,k}$ and $u_{1,k}$ are set equal to $e_{AD,C}\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$ and $e_{AD,1}\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$ respectively, where $e_{AD,C}$  and $e_{AD,1}$ are constants set such that the probability of rejecting either hypothesis under the global null hypothesis is zero. Specific decision rules based on these boundaries for the z-statistics are described later on in this section. 

The boundaries for stopping the $AD$ design without rejecting the null hypotheses are denoted as $l_{1,k}$ and $l_{2,k}$. These stopping boundaries are defined relative to the test statistics $Z_{1,k}$ and $Z_{2,k}$. The boundaries are set equal to $f_{AD,2}\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$ and $f_{AD,1}\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$ respectively, where $f_{AD,2}$ and $f_{AD,1}$ can be set by the user. In each stage, our adaptive design has the option of stopping enrollment in subpopulation 2, based on the treatment effect estimate $Z_{2,k}$, but continuing to enroll from subpopulation 1. 

The second way that the decision boundaries of $AD$ differ from those of the standard designs is that *interAdapt* allows more flexibility in the futility boundaries.  Specifically, *interAdapt* allows the user to specify a final stage for testing an effect in the total population, denoted by stage $k^\star$. Regardless of the results at stage $k^\star$, we always stop enrolling from subpopulation $2$ at the end stage $k^\star$, if we have not done so already. The futility boundaries $l_{2,k}$ are not defined for $k>k^\star$.


For the $AD$ design, the user can specify two stage specific sample sizes, one for stages when both populations are enrolled $(k \leq k^\star)$, and one for stages where only patients in subpopulation 1 are enrolled $(k > k^\star)$. We refer to these two sample sizes as $n_1^\star$ and $n_k^\star$ respectively.

As described in `r citep(bibFile[['Rosenblum2013AdaptMISTIE']])`, our decision rules in $AD$ consist of the following steps for each stage $k$:


* 1. (Assess Efficacy) If $Z_{C,k} > u_{C,k}$, reject $H_{0C}$. If $Z_{1,k}>u_{1,k}$, reject $H_{01}$. If either, or both null hypothesis are rejected, stop all enrollment and end the trial.
* 2. (Assess Futility of the entire trial) Else, if $Z_{1,k} ≤ l_{1,k}$ or if this is the final stage of the trial, stop all enrollment and end the trial for futility, failing to reject either $H_{0C}$ or $H_{01}$.
* 3. (Assess Futility for $H_{0C}$) Else, if $Z_{2,k} ≤ l_{2,k}$, or if $k\geq k^\star$, stop enrollment from subpopulation $2$ in all future stages. In this case, the following steps must then be done:
  *  3.a If $Z_{1,k} > u_{1,k}$, reject $H_{01}$ and stop all enrollment.
	*  3.b If $Z_{1,k} ≤ l_{1,k}$ or if this is the final stage of the trial, conclude that we've fail to reject either $H_{0C}$ or $H_{01}$, and stop all enrollment.
	*  3.c Else, continue by enrolling from subpopulation $1$. If $k < k^\star$ then $π_1n_1^\star$ patients should be enrolled in the next stage. If $k \geq k^\star$, then $n_k^\star$ patients should be enrolled in the next stage. for the next stage. For all future stages, ignore steps (1-2) and proceed directly to steps (3.a-3.c).
*  4. (Continue Enrollment from Combined Population) Else, continue by enrolling $\pi_1 n_1^\star$ participants from subpopulation 1 and $\pi_2 n_1^\star$ participants from subpopulation 2 for the next stage.


The decision rules outputted by *interAdapt* represent the feature that enrollment of subpopulation 2 cannot continue after stage $k^\star$ by setting the futility boundary $l_{2,k^\star}$ equal to infinity. This ensures that $Z_{2,k^\star} < l_{2,k^\star}$. 

To correctly calibrate $e_{AD,C}$  and $e_{AD,1}$, *interAdapt* first chooses $e_{AD,C}$ such the probability of falsely rejecting $H_{0C}$ is $a_c α$, where $a_c$ is a fraction between 0 and 1 that can be specified by the user. Then, conditional on $e_{AD,C}$, *interAdapt* finds the smallest constant $e_{AD,1}$ such that, under the global null of no treatment effect in either subpopulation, we have

\[
P \left(
Z_{C,k}>e_{AD,C} \left\{\frac{\sum_{k'=1}^{K} n_{k'}}{\sum_{k'=1}^{k}n_{k'}}\right\}^{-δ} \text{  or  } 
Z_{1,k}> e_{AD,1}\left\{\frac{\sum_{k'=1}^{K} n_{k'}}{\sum_{k'=1}^{k}n_{k'}}\right\}^{-δ}\text{  for any $k$}
\right) ≤ α 
\]

The fact that familywise Type I error rate is controlled under the global null implies that it is also strongly controlled under all hypotheses `r citep(bibFile[['Rosenblum2013AdaptMISTIE']])`. 





*************






# <a name="Inputs">Inputs</a>

## <a name="Basic Parameters">Basic Parameters</a>

* Subpopulation $1$ proportion ($π_1$): The proportion of the population in subpopulation $1$. This is the subpopulation in which we have prior evidence of a stronger treatment effect. 

* Probability outcome = 1 under control, subpopulation $1$ ($p_{1c}$): The probability of experiencing a successful outcome for control participants in subpopulation $1$. This is used in estimating power and expected sample size of each design.

* Probability outcome = 1 under control, subpopulation $2$ ($p_{2c}$): The probability of experiencing a successful outcome for control participants in subpopulation $2$. This is used in estimating power and expected sample size of each design.

* Probability outcome = 1 under treatment for subpopulation $1$ ($p_{1t}$): The probability of experiencing a successful outcome for treated participants in subpopulation $1$. Note that a specific treatment effect size is not specified for subpopulation $2$. Instead, *interAdapt* generates the relevant performance metrics for a range of several possible effect sizes in subpopulation $2$. This range can be specified in the Advanced Parameters section.

* Per stage sample size, combined population, for adaptive design ($n_1^\star$): Number of patients enrolled per stage in $AD$, whenever both subpopulations are being enrolled.

* Per stage sample size for stages where only subpopulation 1 is enrolled, for adaptive design ($n_k^\star$): The number of patients required for each stage after stage $k^\star$. For stages up to and including stage $k^\star$, the number of patients enrolled from subpopulation 1 is equal to $\pi_1 n_1^\star$.


* Alpha (FWER) requirement for all designs ($α$): The rate familywise Type I error rate for all hypotheses in the trial. In $AD$, this is the probability of falsely rejecting either $H_{0C}$ or $H_{01}$. In $SC$ it is the probability of falsely rejecting $H_{0C}$. In $SS$ it is the probability of falsely rejecting $H_{01}$.

* Proportion of Alpha allocated to H0C for adaptive design ($a_C$): To control the familywise Type I error rate in the $AD$ design, the test of $H_{0C}$ is first calibrated to have a Type I error rate equal to $a_Cα$. The decision rules for $H_{01}$ are then calibrated so that the overall familywise Type I error rate is equal to $α$.


## <a name="Advanced Parameters">Advanced Parameters</a>

* Delta (δ): This parameter defines the curvature of the efficacy and futility boundaries, which are all proportional to $\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$. 

* \# of Iterations for simulation: Z-statistics are simulated generate the power, expected sample size, and expected trial duration. Generally, about 10,000 simulations are needed for reliable results. It is our experience that a simulation with 10,000 iterations takes about 7-15 seconds on a commercial laptop.

* Time limit for simulation, in seconds: If the simulation time exceeds this threshold, calculations will stop and the user will get an error message saying that the application has “reached CPU time limit”. To remove the error, either the number of iterations can be reduced, or the time limit for simulation can be extended. *interAdapt* does not allow for this time limit to exceed 90 seconds.

* Total number of stages ($K$): The total number of stages for all three designs. 

* Last stage subpopulation $2$ is enrolled under adaptive design ($k^\star$): In the adaptive design, we don’t enroll any participants from subpopulation $2$ after stage $k^\star$. 

* Participants enrolled per year from combined population: The number of participants that can be recruited per year in the combined population. This affects the estimated duration of the trials. The enrollment rates for  subpopulations $1$ and $2$ are equal to the combined population enrollment rate multiplied by $π_1$ and $π_2$ respectively. Active enrollment from one subpopulation is assumed to have no affect on the enrollment rate in the other subpopulation. 


* Per stage sample size for standard group sequential design (SC) enrolling combined pop. ($n_{SC}$): The number of participants enrolled in each stage for $SC$.

* Per stage sample size for standard group sequential design (SS) enrolling only subpop. 1 ($n_{SS}$): The number of participants enrolled in each stage for $SS$.

* Stopping boundary proportionality constant for subpopulation 2 enrollment for adaptive design ($f_{AD,2}$): This is used to calculate the futility boundary ($l_{2,k})$ for the z-statistics calculated in subpopulation 2 ($Z_{2,k}$). The boundary for stage $k$ is set equal to $l_{2,k}=f_{AD,2}\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$. If $Z_{2,k}\leq l_{2,k}$, we stop enrollment of subpopulation 2.

* $H_{01}$ futility boundary proportionality constant for the adaptive design ($f_{AD,1}$):  This is used to calculate the futility boundary ($l_{1,k}$) for the z-statistics calculated in subpopulation 1 ($Z_{1,k}$). The boundary for stage $k$ is set to  $l_{1,k}=f_{AD,1}\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$.  If $Z_{1,k}\leq l_{1,k}$, we stop all enrollment.

* $H_{0C}$ futility boundary proportionality constant for the standard design ($f_{SC}$): This is used to calculate the futility boundary for $H_{0C}$ in $SC$, which is set to $f_{SC}\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$ in stage $k$.

* $H_{01}$ futility boundary proportionality constant for the standard design ($f_{SS}$):  This is used to calculate the futility boundary for $H_{01}$ in $SS$, which is set to $f_{SS}\{(\sum_{k'=1}^{K} n_{k'})/(\sum_{k'=1}^{k}n_{k'})\}^{-δ}$ in stage $k$.


* Lowest value to plot for treatment effect in subpopulation 2: *interAdapt* simulates performance metrics under a range of treatment effect sizes for subpopulation $2$. This sets the lower bound for this range.

* Greatest value to plot for treatment effect in subpopulation 2: *interAdapt* simulates performance metrics under a range of treatment effect sizes for subpopulation $2$. This sets the upper bound for this range.





**********








# <a name="References">References</a>

This report was created using the *knitr* R package `r citep(bibFile[["knitr"]])`, with citations created using the *knitcitations* R package `r citep(bibFile[["knitcitations"]])`.


```{r}
## Print bibliography
bibliography()
```






